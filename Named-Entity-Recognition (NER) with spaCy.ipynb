{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Named Entity Recognition (NER) withÂ Spacy\n",
    "\n",
    "The purpose of this notebook is to demonstrate the entire process of name-entity recognition([**NER**](https://nlp.stanford.edu/ner/)) from start to the end with [**Spacy**](https://spacy.io/). \n",
    "This notebook also explore **pattern matching** as an alternative to **NER** when there is a known small set of fixed values. \n",
    "\n",
    "This will be a complete end-to-end demonstration of the entire process, including both labelling and model training.\n",
    "\n",
    "In this notebook, we train a model to detect entities related to **oil/petrol** from this [public dataset](https://www.kaggle.com/mitusha/email-dataset) which contains a list of emails related to the oil industry. This is an over simplification because we want more generic entities, but it shows how pattern matching is a better alternative than NER in this case. To summarise, we will extract oil-related elements from email messages.\n",
    "\n",
    "Below are the process perform in this notebook:\n",
    "- Read the emails data set which has an email per line.\n",
    "- Label the emails with the OIL entity using **[Doccano](http://doccano.herokuapp.com/)** labeling tool. This is a manual process.\n",
    "- Save the labels in a text file as **JSONL**.\n",
    "- Use **Spacy** Neural Network model to train a new statistical model. \n",
    "- Save the model.\n",
    "- Create a **Spacy** NLP pipeline and use the new model to detect oil entities never seen before.\n",
    "- Use pattern matching instead of a deep learning model to compare both method.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Label the Data\n",
    "\n",
    "First Step: Level the data using open source platform **Doccano**.\n",
    "\n",
    "Follow **[Doccano](https://doccano.github.io/doccano/tutorial/)** instructions to install and open Doccano.\n",
    "\n",
    "If you use **Linux/Mac**, I recommend using the docker image:\n",
    "- `docker pull doccano/doccano`\n",
    "- `docker container create --name doccano -e \"ADMIN_USERNAME=admin\" -e \"ADMIN_EMAIL=admin@example.com\" -e \"ADMIN_PASSWORD=password\" -p 8000:8000 doccano/doccano`\n",
    "- `docker container start doccano`\n",
    "    \n",
    "For **Windows**, just use **pip**: \n",
    "- `pip install doccano`\n",
    "- `doccano`\n",
    "\n",
    "Go to http://127.0.0.1:8000/.\n",
    "\n",
    "Next, label the data using Doccano. Find entities which talk about oil, petrol, petroleum, etc and label them with the tag **OIL**. \n",
    "\n",
    "Export the result as **JSONL(Text label)** format."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "First, let's read the JSONL file using format:\n",
    "\n",
    "`{\"id\": 15, \"text\": \"....\", \"meta\": {}, \"annotation_approver\": null, \"labels\": [[226, 234, \"OIL\"]]}\n",
    "`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "labeled_data = []\n",
    "with open(r\"emails_labeled.jsonl\", \"r\") as read_file:\n",
    "    for line in read_file:\n",
    "        data = json.loads(line)\n",
    "        labeled_data.append(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Convert the format to spacy format\n",
    "\n",
    "Next, let's convert the Deccano format to Spacy format.\n",
    "\n",
    "We will also remove extra columns and rename labels to entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAINING_DATA = []\n",
    "for entry in labeled_data:\n",
    "    entities = []\n",
    "    for e in entry['labels']:\n",
    "        entities.append((e[0], e[1],e[2]))\n",
    "    spacy_entry = (entry['text'], {\"entities\": entities})\n",
    "    TRAINING_DATA.append(spacy_entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model!\n",
    "\n",
    "To avoid overfitting, we use Deep Learning (NN) with a 0.3 dropout rate.\n",
    "\n",
    "The concept is to employ a Neural Network with multiple layers and many neurons. We show them text that has previously been classified, so we already know the answer. We'll run numerous iterations, and on each one, we'll calculate the error using a Loss Function, which will cause the weight of the neurons to be adjusted, causing them to activate. As time goes on, the network's weight will be adjusted to reduce incorrect learning patterns and solve the problem.\n",
    "\n",
    "We randomly delete certain neurons on each iteration to avoid overfitting, which implies the model \"memorises\" the training data and does not perform well with fresh data. This allows the model to generalise better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember to install Spacy first:\n",
    "```\n",
    "pip install -U pip setuptools wheel\n",
    "pip install -U spacy\n",
    "python -m spacy download en_core_web_sm\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/risehill/opt/anaconda3/lib/python3.8/site-packages/spacy/language.py:635: UserWarning: [W033] Training a new parser or NER using a model with an empty lexeme normalization table. This may degrade the performance to some degree. If this is intentional or this language doesn't have a normalization table, please ignore this warning.\n",
      "  proc.begin_training(\n",
      "/Users/risehill/opt/anaconda3/lib/python3.8/site-packages/spacy/language.py:635: UserWarning: [W034] Please install the package spacy-lookups-data in order to include the default lexeme normalization table for the language 'en'.\n",
      "  proc.begin_training(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ner': 1973.4865596213326}\n",
      "{'ner': 113.57156751269132}\n",
      "{'ner': 217.56983780162102}\n",
      "{'ner': 135.77832285394533}\n",
      "{'ner': 18.947545839105487}\n",
      "{'ner': 2.3462759127285975}\n",
      "{'ner': 2.7424490023819144}\n",
      "{'ner': 0.2527100517836849}\n",
      "{'ner': 1.9989370382904463}\n",
      "{'ner': 2.4769654040015526e-06}\n",
      "{'ner': 6.686036946592199e-05}\n",
      "{'ner': 4.48429154853934e-07}\n",
      "{'ner': 2.35069317865294e-06}\n",
      "{'ner': 4.132060679846962e-06}\n",
      "{'ner': 3.0780311702707956e-05}\n",
      "{'ner': 4.0529153954865305e-06}\n",
      "{'ner': 8.623072892827798e-08}\n",
      "{'ner': 2.3936120363095938e-08}\n",
      "{'ner': 9.26201417283012e-07}\n",
      "{'ner': 6.676562402811059e-05}\n",
      "{'ner': 0.8652161449037427}\n",
      "{'ner': 3.6356021931840714}\n",
      "{'ner': 0.6382250019502252}\n",
      "{'ner': 0.7674873296283516}\n",
      "{'ner': 3.059514327155964e-05}\n",
      "{'ner': 0.009003881051844805}\n",
      "{'ner': 0.00015068832124390008}\n",
      "{'ner': 0.3304007721099634}\n",
      "{'ner': 1.6587583378080304e-10}\n",
      "{'ner': 1.8754305088359436e-05}\n",
      "{'ner': 2.1138066257901233e-11}\n",
      "{'ner': 8.570752098763121e-10}\n",
      "{'ner': 2.6192045906057025e-10}\n",
      "{'ner': 1.1126011079646339e-10}\n",
      "{'ner': 2.0228323421276887e-09}\n",
      "{'ner': 3.8307377558269323e-10}\n",
      "{'ner': 5.360561217764215e-09}\n",
      "{'ner': 7.752725084996623e-06}\n",
      "{'ner': 3.308759186154706e-11}\n",
      "{'ner': 1.4262378215858835}\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "import random\n",
    "import json\n",
    "\n",
    "nlp = spacy.blank(\"en\")\n",
    "ner = nlp.create_pipe(\"ner\")\n",
    "nlp.add_pipe(ner)\n",
    "ner.add_label(\"OIL\")\n",
    "\n",
    "# Start the training\n",
    "nlp.begin_training()\n",
    "\n",
    "# Loop for 40 iterations\n",
    "for itn in range(40):\n",
    "    # Shuffle the training data\n",
    "    random.shuffle(TRAINING_DATA)\n",
    "    losses = {}\n",
    "\n",
    "    # Batch the examples and iterate over them\n",
    "    for batch in spacy.util.minibatch(TRAINING_DATA, size=2):\n",
    "        texts = [text for text, entities in batch]\n",
    "        annotations = [entities for text, entities in batch]\n",
    "\n",
    "        # Update the model\n",
    "        nlp.update(texts, annotations, losses=losses, drop=0.3)\n",
    "    print(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see the error decreasing as iterations go by, note that some times it may increase due to the dropout setting.\n",
    "\n",
    "#### Save the model to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk(\"oil.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model\n",
    "\n",
    "Let's test the model.  For this we use displacy which will display the entities in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "example = \"service postings marathon petroleum co said it reduced the contract price it will pay for all grades of service oil one dlr a barrel effective today the decrease brings marathon s posted price for both west texas intermediate and west texas sour to dlrs a bbl the south louisiana sweet grade of service was reduced to dlrs a bbl the company last changed its service postings on jan reuter\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">service postings marathon \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    petroleum\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">OIL</span>\n",
       "</mark>\n",
       " co said it reduced the contract price it will pay for all grades of service \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    oil\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">OIL</span>\n",
       "</mark>\n",
       " one dlr a barrel effective today the decrease brings marathon s posted price for both west texas intermediate and west texas sour to dlrs a bbl the south louisiana sweet grade of service was reduced to dlrs a bbl the company last changed its service postings on jan reuter</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "doc = nlp(example)\n",
    "displacy.render(doc, style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "We have shown how to label data with **Doccano** and create a custom model with **Spacy**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phrase Matching\n",
    "\n",
    "The second approach is to use pattern matching to look for certain keywords and patterns in the text. \n",
    "\n",
    "**Spacy** provides matchers which can be easily used to look for specific substrings, digits, etc. We can also set rules based on the part-of-speech tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matches: ['petroleum', 'oil']\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "# Import the Matcher\n",
    "from spacy.matcher import Matcher\n",
    "\n",
    "nlp = spacy.load(name='en_core_web_sm')\n",
    "doc = nlp(example)\n",
    "\n",
    "# Initialize the Matcher with the shared vocabulary\n",
    "matcher = Matcher(nlp.vocab)\n",
    "\n",
    "# Add the pattern to the matcher\n",
    "matcher.add(\"OIL_PATTERN\", None, [{\"LOWER\": \"oil\"}], [{\"LOWER\": \"petroleum\"}])\n",
    "\n",
    "# Use the matcher on the doc\n",
    "matches = matcher(doc)\n",
    "print(\"Matches:\", [doc[start:end].text for match_id, start, end in matches])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see how we also found the right tag, but in this case typos or similar words are not detected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "We have seen to different approaches used for entity recognition. \n",
    "\n",
    "**Pattern Matching** can be used in the following use cases:\n",
    "- Low cardinality attributes\n",
    "- Common patterns such dates, quantities, numbers, etc.\n",
    "- Patterns occuring in certain parts of the speech\n",
    "- When typos are not expected\n",
    "- Structured data\n",
    "\n",
    "\n",
    "\n",
    "**Statistical Models** are great to learn complex patterns in the data and can \"guess\" and categorize data never seem before. Use cases:\n",
    "- High Cardinality attributes\n",
    "- You need to deal with typos (fuzzy matches)\n",
    "- You need to categorize new, never seen data.\n",
    "- Unstructured data\n",
    "\n",
    "These models are much more powerful since they can make decisions on things that were never trained on. It can detect new entities without any code change.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
